{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PranavSingla122/Fatty-Liver-Analysis/blob/main/fattyliver.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "# Load the datasets\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/fatty liver/train.csv')  # Update with actual URL\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/fatty liver/test.csv')  # Update with actual URL\n",
        "\n",
        "# Drop unnecessary columns\n",
        "drop_columns = ['Unnamed: 0', 'FBID', 'SampleID', 'Fibrosis', 'group']\n",
        "train_df = train_df.drop(columns=[col for col in drop_columns if col in train_df.columns])\n",
        "test_df = test_df.drop(columns=[col for col in drop_columns if col in test_df.columns])\n",
        "\n",
        "# Ensure all columns are numeric\n",
        "train_df = train_df.apply(pd.to_numeric, errors='coerce')\n",
        "test_df = test_df.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Drop rows with missing values\n",
        "train_df = train_df.dropna()\n",
        "test_df = test_df.dropna()\n",
        "\n",
        "# Encode categorical variable 'Sex' consistently across train and validation sets\n",
        "label_encoder = LabelEncoder()\n",
        "train_df['Sex'] = label_encoder.fit_transform(train_df['Sex'])\n",
        "test_df['Sex'] = label_encoder.transform(test_df['Sex'])\n",
        "\n",
        "# Check if 'label' column exists, otherwise create it\n",
        "if 'label' not in train_df.columns:\n",
        "    train_df['label'] = (train_df['BMI'] > 25).astype(int)  # Example threshold for fatty liver\n",
        "if 'label' not in test_df.columns:\n",
        "    test_df['label'] = (test_df['BMI'] > 25).astype(int)  # Ensure same logic for validation set\n",
        "\n",
        "# Extract features and labels\n",
        "X_train = train_df.drop(columns=['label']).values\n",
        "y_train = train_df['label'].values\n",
        "X_test = test_df.drop(columns=['label']).values\n",
        "y_test = test_df['label'].values\n",
        "\n",
        "# Normalize the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define PyTorch Dataset class\n",
        "class LiverDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# Create DataLoaders\n",
        "train_dataset = LiverDataset(X_train, y_train)\n",
        "test_dataset = LiverDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define Neural Network class\n",
        "class LiverClassifier(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(LiverClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 128)\n",
        "        self.bn1 = nn.BatchNorm1d(128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.bn2 = nn.BatchNorm1d(64)\n",
        "        self.fc3 = nn.Linear(64, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.bn1(self.fc1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.bn2(self.fc2(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)  # Removed sigmoid activation\n",
        "        return x\n",
        "\n",
        "# Model initialization\n",
        "input_size = X_train.shape[1]\n",
        "model = LiverClassifier(input_size)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 5\n",
        "best_loss = np.inf\n",
        "counter = 0\n",
        "\n",
        "# Training loop\n",
        "def train_model(model, train_loader, test_loader, criterion, optimizer, scheduler, epochs=50):\n",
        "    global best_loss, counter\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X_batch).squeeze()\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        val_loss = evaluate_model(model, test_loader)\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "        # Early stopping\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            counter = 0\n",
        "        else:\n",
        "            counter += 1\n",
        "            if counter >= patience:\n",
        "                print(\"Early stopping triggered\")\n",
        "                break\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in test_loader:\n",
        "            outputs = model(X_batch).squeeze()\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            total_loss += loss.item()\n",
        "            predictions = (torch.sigmoid(outputs) > 0.5).float()\n",
        "            correct += (predictions == y_batch).sum().item()\n",
        "            total += y_batch.size(0)\n",
        "    val_loss = total_loss / len(test_loader)\n",
        "    accuracy = correct / total\n",
        "    print(f\"Testing Accuracy: {accuracy:.4f}\")\n",
        "    return val_loss\n",
        "\n",
        "# Train and evaluate\n",
        "train_model(model, train_loader, test_loader, criterion, optimizer, scheduler)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbH0zv70HCDk",
        "outputId": "40a8aaf7-51c2-42e8-926f-20f6d94f4d7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing Accuracy: 0.8027\n",
            "Epoch 1/50, Loss: 0.6280, Val Loss: 0.6059\n",
            "Testing Accuracy: 0.9184\n",
            "Epoch 2/50, Loss: 0.4848, Val Loss: 0.5067\n",
            "Testing Accuracy: 0.9252\n",
            "Epoch 3/50, Loss: 0.4037, Val Loss: 0.4391\n",
            "Testing Accuracy: 0.9388\n",
            "Epoch 4/50, Loss: 0.3454, Val Loss: 0.3871\n",
            "Testing Accuracy: 0.9320\n",
            "Epoch 5/50, Loss: 0.3088, Val Loss: 0.3568\n",
            "Testing Accuracy: 0.9320\n",
            "Epoch 6/50, Loss: 0.2662, Val Loss: 0.3369\n",
            "Testing Accuracy: 0.9388\n",
            "Epoch 7/50, Loss: 0.2500, Val Loss: 0.3138\n",
            "Testing Accuracy: 0.9456\n",
            "Epoch 8/50, Loss: 0.2336, Val Loss: 0.2888\n",
            "Testing Accuracy: 0.9456\n",
            "Epoch 9/50, Loss: 0.2412, Val Loss: 0.2754\n",
            "Testing Accuracy: 0.9456\n",
            "Epoch 10/50, Loss: 0.2122, Val Loss: 0.2763\n",
            "Testing Accuracy: 0.9524\n",
            "Epoch 11/50, Loss: 0.1830, Val Loss: 0.2610\n",
            "Testing Accuracy: 0.9320\n",
            "Epoch 12/50, Loss: 0.2071, Val Loss: 0.2525\n",
            "Testing Accuracy: 0.9388\n",
            "Epoch 13/50, Loss: 0.1979, Val Loss: 0.2505\n",
            "Testing Accuracy: 0.9320\n",
            "Epoch 14/50, Loss: 0.1692, Val Loss: 0.2470\n",
            "Testing Accuracy: 0.9456\n",
            "Epoch 15/50, Loss: 0.1548, Val Loss: 0.2171\n",
            "Testing Accuracy: 0.9116\n",
            "Epoch 16/50, Loss: 0.1591, Val Loss: 0.2241\n",
            "Testing Accuracy: 0.9388\n",
            "Epoch 17/50, Loss: 0.1497, Val Loss: 0.2102\n",
            "Testing Accuracy: 0.9388\n",
            "Epoch 18/50, Loss: 0.1579, Val Loss: 0.2093\n",
            "Testing Accuracy: 0.9456\n",
            "Epoch 19/50, Loss: 0.1479, Val Loss: 0.1973\n",
            "Testing Accuracy: 0.9320\n",
            "Epoch 20/50, Loss: 0.1479, Val Loss: 0.2005\n",
            "Testing Accuracy: 0.9388\n",
            "Epoch 21/50, Loss: 0.1379, Val Loss: 0.1916\n",
            "Testing Accuracy: 0.9252\n",
            "Epoch 22/50, Loss: 0.1370, Val Loss: 0.1902\n",
            "Testing Accuracy: 0.9524\n",
            "Epoch 23/50, Loss: 0.1291, Val Loss: 0.1839\n",
            "Testing Accuracy: 0.9524\n",
            "Epoch 24/50, Loss: 0.1190, Val Loss: 0.1843\n",
            "Testing Accuracy: 0.9524\n",
            "Epoch 25/50, Loss: 0.1362, Val Loss: 0.1750\n",
            "Testing Accuracy: 0.9320\n",
            "Epoch 26/50, Loss: 0.1571, Val Loss: 0.1898\n",
            "Testing Accuracy: 0.9524\n",
            "Epoch 27/50, Loss: 0.1137, Val Loss: 0.1838\n",
            "Testing Accuracy: 0.9388\n",
            "Epoch 28/50, Loss: 0.1042, Val Loss: 0.1811\n",
            "Testing Accuracy: 0.9524\n",
            "Epoch 29/50, Loss: 0.1424, Val Loss: 0.1685\n",
            "Testing Accuracy: 0.9524\n",
            "Epoch 30/50, Loss: 0.1138, Val Loss: 0.1651\n",
            "Testing Accuracy: 0.9592\n",
            "Epoch 31/50, Loss: 0.1344, Val Loss: 0.1651\n",
            "Testing Accuracy: 0.9592\n",
            "Epoch 32/50, Loss: 0.1071, Val Loss: 0.1605\n",
            "Testing Accuracy: 0.9456\n",
            "Epoch 33/50, Loss: 0.1138, Val Loss: 0.1676\n",
            "Testing Accuracy: 0.9320\n",
            "Epoch 34/50, Loss: 0.1059, Val Loss: 0.1722\n",
            "Testing Accuracy: 0.9456\n",
            "Epoch 35/50, Loss: 0.1070, Val Loss: 0.1734\n",
            "Testing Accuracy: 0.9388\n",
            "Epoch 36/50, Loss: 0.1007, Val Loss: 0.1771\n",
            "Testing Accuracy: 0.9388\n",
            "Epoch 37/50, Loss: 0.1067, Val Loss: 0.1756\n",
            "Early stopping triggered\n"
          ]
        }
      ]
    }
  ]
}